{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "from tdc import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data\n",
    "train_data = pd.read_csv('../../data/MorganCount/tox21_train_featurized.csv').dropna()\n",
    "valid_data = pd.read_csv('../../data/MorganCount/tox21_valid_featurized.csv').dropna()\n",
    "test_data = pd.read_csv('../../data/MorganCount/tox21_test_featurized.csv').dropna()\n",
    "\n",
    "#get splits\n",
    "X_train, y_train = train_data.filter(regex='^dim_.*'), train_data['Y']\n",
    "X_test, y_test = test_data.filter(regex='^dim_.*'), test_data['Y']\n",
    "X_valid, y_valid = valid_data.filter(regex='^dim_.*'), valid_data['Y']\n",
    "\n",
    "#Use smote to oversample minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=7,               # Balance complexity\n",
    "    learning_rate=0.05,        # Slower learning\n",
    "    scale_pos_weight=8,        # Adjust for class imbalance \n",
    "    random_state=42,\n",
    "    early_stopping_rounds=10,   #prevent overfitting\n",
    ")\n",
    "\n",
    "model.fit(X_res, y_res , eval_set=[(X_valid, y_valid)], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.87      0.90       979\n",
      "         1.0       0.49      0.69      0.57       181\n",
      "\n",
      "    accuracy                           0.84      1160\n",
      "   macro avg       0.71      0.78      0.74      1160\n",
      "weighted avg       0.87      0.84      0.85      1160\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.85      0.90       472\n",
      "         1.0       0.55      0.78      0.65       109\n",
      "\n",
      "    accuracy                           0.84       581\n",
      "   macro avg       0.75      0.82      0.77       581\n",
      "weighted avg       0.87      0.84      0.85       581\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get prediction result\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_val_pred = model.predict(X_valid)\n",
    "\n",
    "# Display metrics\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(classification_report(y_valid, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.8859\n",
      "PR-AUC: 0.6369\n",
      "Accuracy: 0.8509\n",
      "Precision: 0.5161\n",
      "Recall: 0.7072\n",
      "F1: 0.5967\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any\n",
    "def evaluate_model(y_true, y_pred_proba, threshold: float = 0.5) -> Dict[str, float]:\n",
    "    metrics = {\n",
    "        'ROC-AUC': {'name': 'ROC-AUC', 'kwargs': {}},\n",
    "        'PR-AUC': {'name': 'PR-AUC', 'kwargs': {}},\n",
    "        'Accuracy': {'name': 'Accuracy', 'kwargs': {'threshold': threshold}},\n",
    "        'Precision': {'name': 'Precision', 'kwargs': {'threshold': threshold}},\n",
    "        'Recall': {'name': 'Recall', 'kwargs': {'threshold': threshold}},\n",
    "        'F1': {'name': 'F1', 'kwargs': {'threshold': threshold}}\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for metric_name, config in metrics.items():\n",
    "        evaluator = Evaluator(name=config['name'])\n",
    "        score = evaluator(y_true, y_pred_proba, **config['kwargs'])\n",
    "        results[metric_name] = score\n",
    "        print(f\"{metric_name}: {score:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_true = y_test\n",
    "\n",
    "evaluation_results = evaluate_model(y_true, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Morgan_trained_model.joblib']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save model\n",
    "\n",
    "model = model\n",
    "model_filename = 'Morgan_trained_model.joblib'\n",
    "joblib.dump(model, model_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
