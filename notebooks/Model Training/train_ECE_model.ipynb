{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from tdc import Evaluator\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data\n",
    "train_data = pd.read_csv('../../data/ECE/tox21_train_featurized.csv').dropna()\n",
    "valid_data = pd.read_csv('../../data/ECE/tox21_valid_featurized.csv').dropna()\n",
    "test_data = pd.read_csv('../../data/ECE/tox21_test_featurized.csv').dropna()\n",
    "\n",
    "#get splits\n",
    "X_train, y_train = train_data.filter(regex='^feature.*'), train_data['Y']\n",
    "X_test, y_test = test_data.filter(regex='^feature.*'), test_data['Y']\n",
    "X_valid, y_valid = valid_data.filter(regex='^feature.*'), valid_data['Y']\n",
    "\n",
    "#Use smote to oversample minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "model = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=7,               # Balance complexity\n",
    "    learning_rate=0.05,        # Slower learning\n",
    "    gamma=0.5,                 # Regularization\n",
    "    subsample=0.8,             # Reduce overfitting\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=10,        # Adjust for class imbalance \n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "model.fit(X_res, y_res , eval_set=[(X_valid, y_valid)], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.90      0.92       981\n",
      "         1.0       0.57      0.73      0.64       181\n",
      "\n",
      "    accuracy                           0.87      1162\n",
      "   macro avg       0.76      0.82      0.78      1162\n",
      "weighted avg       0.89      0.87      0.88      1162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict test data\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Display metrics\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.8897\n",
      "PR-AUC: 0.6754\n",
      "Accuracy: 0.8718\n",
      "Precision: 0.5684\n",
      "Recall: 0.7348\n",
      "F1: 0.6410\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any\n",
    "def evaluate_model(y_true, y_pred_proba, threshold: float = 0.5) -> Dict[str, float]:\n",
    "    metrics = {\n",
    "        'ROC-AUC': {'name': 'ROC-AUC', 'kwargs': {}},\n",
    "        'PR-AUC': {'name': 'PR-AUC', 'kwargs': {}},\n",
    "        'Accuracy': {'name': 'Accuracy', 'kwargs': {'threshold': threshold}},\n",
    "        'Precision': {'name': 'Precision', 'kwargs': {'threshold': threshold}},\n",
    "        'Recall': {'name': 'Recall', 'kwargs': {'threshold': threshold}},\n",
    "        'F1': {'name': 'F1', 'kwargs': {'threshold': threshold}}\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for metric_name, config in metrics.items():\n",
    "        evaluator = Evaluator(name=config['name'])\n",
    "        score = evaluator(y_true, y_pred_proba, **config['kwargs'])\n",
    "        results[metric_name] = score\n",
    "        print(f\"{metric_name}: {score:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_true = y_test\n",
    "\n",
    "evaluation_results = evaluate_model(y_true, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ECE_trained_model.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save model using joblib\n",
    "import joblib\n",
    "\n",
    "model = model\n",
    "model_filename = 'ECE_trained_model.joblib' #ECE = Ersilia Compound Embeddings\n",
    "joblib.dump(model, model_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
