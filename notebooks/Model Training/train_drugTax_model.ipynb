{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "from flaml import AutoML\n",
    "from sklearn.utils import compute_sample_weight\n",
    "from tdc import Evaluator\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data\n",
    "columns_to_drop = ['Drug_ID', 'Drug', 'Y', 'key', 'input']\n",
    "train_data = pd.read_csv('../../data/DrugTax/train_drugTax_featurized.csv').dropna()\n",
    "valid_data = pd.read_csv('../../data/DrugTax/valid_drugTax_featurized.csv').dropna()\n",
    "test_data = pd.read_csv('../../data/DrugTax/test_drugTax_featurized.csv').dropna()\n",
    "\n",
    "#get splits\n",
    "X_train, y_train = train_data.drop(columns=columns_to_drop).filter(regex='^(?!char_[.,=#@+\\\\-\\\\[\\\\(\\\\\\\\\\/])'), train_data['Y']\n",
    "X_test, y_test = test_data.drop(columns=columns_to_drop).filter(regex='^(?!char_[.,=#@+\\\\-\\\\[\\\\(\\\\\\\\\\/])'), test_data['Y']\n",
    "X_valid, y_valid = valid_data.drop(columns=columns_to_drop).filter(regex='^(?!char_[.,=#@+\\\\-\\\\[\\\\(\\\\\\\\\\/])'), valid_data['Y']\n",
    "\n",
    "#Use smote to oversample minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model using Flaml AutoML\n",
    "\n",
    "model_config = {\n",
    "    'task' : 'classification',  # classification \n",
    "    'time_budget' : 300,    # time budget in seconds\n",
    "    'metric' : 'f1', # main metric to be optimized\n",
    "    'estimator_list' : ['lgbm', 'xgboost', 'rf'] ,\n",
    "}\n",
    "\n",
    "model = AutoML()\n",
    "model.fit(X_res, y_res, **model_config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lgbm': {'n_estimators': 86,\n",
       "  'num_leaves': 66,\n",
       "  'min_child_samples': 10,\n",
       "  'learning_rate': 0.4237053714802325,\n",
       "  'log_max_bin': 10,\n",
       "  'colsample_bytree': 0.607391031428273,\n",
       "  'reg_alpha': 0.1352172481077189,\n",
       "  'reg_lambda': 0.002169425895268579},\n",
       " 'xgboost': {'n_estimators': 165,\n",
       "  'max_leaves': 104,\n",
       "  'min_child_weight': 1.2537688969180918,\n",
       "  'learning_rate': 0.10531638435811055,\n",
       "  'subsample': 1.0,\n",
       "  'colsample_bylevel': 0.6357384957771324,\n",
       "  'colsample_bytree': 0.47780094418903957,\n",
       "  'reg_alpha': 0.02922872147319898,\n",
       "  'reg_lambda': 0.3164947839216637},\n",
       " 'rf': {'n_estimators': 57,\n",
       "  'max_features': 0.2240610615176969,\n",
       "  'max_leaves': 1168,\n",
       "  'criterion': 'entropy'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view the best configuration for each ML model\n",
    "model.best_config_per_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.94      0.92       981\n",
      "         1.0       0.55      0.43      0.48       181\n",
      "\n",
      "    accuracy                           0.86      1162\n",
      "   macro avg       0.73      0.68      0.70      1162\n",
      "weighted avg       0.84      0.86      0.85      1162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict test data\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Display metrics\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.8100\n",
      "PR-AUC: 0.5119\n",
      "Accuracy: 0.8571\n",
      "Precision: 0.5540\n",
      "Recall: 0.4254\n",
      "F1: 0.4813\n"
     ]
    }
   ],
   "source": [
    "#Evaluate model performance\n",
    "from typing import Dict, Any\n",
    "def evaluate_model(y_true, y_pred_proba, threshold: float = 0.5) -> Dict[str, float]:\n",
    "    metrics = {\n",
    "        'ROC-AUC': {'name': 'ROC-AUC', 'kwargs': {}},\n",
    "        'PR-AUC': {'name': 'PR-AUC', 'kwargs': {}},\n",
    "        'Accuracy': {'name': 'Accuracy', 'kwargs': {'threshold': threshold}},\n",
    "        'Precision': {'name': 'Precision', 'kwargs': {'threshold': threshold}},\n",
    "        'Recall': {'name': 'Recall', 'kwargs': {'threshold': threshold}},\n",
    "        'F1': {'name': 'F1', 'kwargs': {'threshold': threshold}}\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for metric_name, config in metrics.items():\n",
    "        evaluator = Evaluator(name=config['name'])\n",
    "        score = evaluator(y_true, y_pred_proba, **config['kwargs'])\n",
    "        results[metric_name] = score\n",
    "        print(f\"{metric_name}: {score:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_true = y_test\n",
    "\n",
    "evaluation_results = evaluate_model(y_true, y_pred_proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
