{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "from flaml import AutoML\n",
    "from tdc import Evaluator\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get data\n",
    "columns_to_drop = ['Drug_ID', 'Drug', 'Y', 'key', 'input']\n",
    "train_data = pd.read_csv('../../data/DrugTax/train_drugTax_featurized.csv').dropna()\n",
    "valid_data = pd.read_csv('../../data/DrugTax/valid_drugTax_featurized.csv').dropna()\n",
    "test_data = pd.read_csv('../../data/DrugTax/test_drugTax_featurized.csv').dropna()\n",
    "\n",
    "#get splits\n",
    "X_train, y_train = train_data.drop(columns=columns_to_drop).filter(regex='^(?!char_[.,=#@+\\\\-\\\\[\\\\(\\\\\\\\\\/])'), train_data['Y']\n",
    "X_test, y_test = test_data.drop(columns=columns_to_drop).filter(regex='^(?!char_[.,=#@+\\\\-\\\\[\\\\(\\\\\\\\\\/])'), test_data['Y']\n",
    "X_valid, y_valid = valid_data.drop(columns=columns_to_drop).filter(regex='^(?!char_[.,=#@+\\\\-\\\\[\\\\(\\\\\\\\\\/])'), valid_data['Y']\n",
    "\n",
    "#Use smote to oversample minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 04-06 21:06:01] {1728} INFO - task = classification\n",
      "[flaml.automl.logger: 04-06 21:06:01] {1739} INFO - Evaluation method: cv\n",
      "[flaml.automl.logger: 04-06 21:06:01] {1838} INFO - Minimizing error metric: 1-f1\n",
      "[flaml.automl.logger: 04-06 21:06:01] {1955} INFO - List of ML learners in AutoML Run: ['lgbm']\n",
      "[flaml.automl.logger: 04-06 21:06:01] {2258} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:01] {2393} INFO - Estimated sufficient time budget=1907s. Estimated necessary time budget=2s.\n",
      "[flaml.automl.logger: 04-06 21:06:01] {2442} INFO -  at 0.6s,\testimator lgbm's best error=0.2905,\tbest estimator lgbm's best error=0.2905\n",
      "[flaml.automl.logger: 04-06 21:06:01] {2258} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:01] {2442} INFO -  at 0.7s,\testimator lgbm's best error=0.2836,\tbest estimator lgbm's best error=0.2836\n",
      "[flaml.automl.logger: 04-06 21:06:01] {2258} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:01] {2442} INFO -  at 0.9s,\testimator lgbm's best error=0.2836,\tbest estimator lgbm's best error=0.2836\n",
      "[flaml.automl.logger: 04-06 21:06:01] {2258} INFO - iteration 3, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:01] {2442} INFO -  at 1.0s,\testimator lgbm's best error=0.2604,\tbest estimator lgbm's best error=0.2604\n",
      "[flaml.automl.logger: 04-06 21:06:01] {2258} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:01] {2442} INFO -  at 1.2s,\testimator lgbm's best error=0.2604,\tbest estimator lgbm's best error=0.2604\n",
      "[flaml.automl.logger: 04-06 21:06:01] {2258} INFO - iteration 5, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:01] {2442} INFO -  at 1.3s,\testimator lgbm's best error=0.2226,\tbest estimator lgbm's best error=0.2226\n",
      "[flaml.automl.logger: 04-06 21:06:01] {2258} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:02] {2442} INFO -  at 1.5s,\testimator lgbm's best error=0.2226,\tbest estimator lgbm's best error=0.2226\n",
      "[flaml.automl.logger: 04-06 21:06:02] {2258} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:02] {2442} INFO -  at 1.6s,\testimator lgbm's best error=0.2226,\tbest estimator lgbm's best error=0.2226\n",
      "[flaml.automl.logger: 04-06 21:06:02] {2258} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:02] {2442} INFO -  at 2.0s,\testimator lgbm's best error=0.2226,\tbest estimator lgbm's best error=0.2226\n",
      "[flaml.automl.logger: 04-06 21:06:02] {2258} INFO - iteration 9, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:02] {2442} INFO -  at 2.1s,\testimator lgbm's best error=0.2226,\tbest estimator lgbm's best error=0.2226\n",
      "[flaml.automl.logger: 04-06 21:06:02] {2258} INFO - iteration 10, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:03] {2442} INFO -  at 2.5s,\testimator lgbm's best error=0.1409,\tbest estimator lgbm's best error=0.1409\n",
      "[flaml.automl.logger: 04-06 21:06:03] {2258} INFO - iteration 11, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:03] {2442} INFO -  at 2.9s,\testimator lgbm's best error=0.0980,\tbest estimator lgbm's best error=0.0980\n",
      "[flaml.automl.logger: 04-06 21:06:03] {2258} INFO - iteration 12, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:03] {2442} INFO -  at 3.2s,\testimator lgbm's best error=0.0980,\tbest estimator lgbm's best error=0.0980\n",
      "[flaml.automl.logger: 04-06 21:06:03] {2258} INFO - iteration 13, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:04] {2442} INFO -  at 3.5s,\testimator lgbm's best error=0.0980,\tbest estimator lgbm's best error=0.0980\n",
      "[flaml.automl.logger: 04-06 21:06:04] {2258} INFO - iteration 14, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:04] {2442} INFO -  at 4.1s,\testimator lgbm's best error=0.0980,\tbest estimator lgbm's best error=0.0980\n",
      "[flaml.automl.logger: 04-06 21:06:04] {2258} INFO - iteration 15, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:05] {2442} INFO -  at 4.9s,\testimator lgbm's best error=0.0943,\tbest estimator lgbm's best error=0.0943\n",
      "[flaml.automl.logger: 04-06 21:06:05] {2258} INFO - iteration 16, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:05] {2442} INFO -  at 5.3s,\testimator lgbm's best error=0.0943,\tbest estimator lgbm's best error=0.0943\n",
      "[flaml.automl.logger: 04-06 21:06:05] {2258} INFO - iteration 17, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:07] {2442} INFO -  at 6.7s,\testimator lgbm's best error=0.0908,\tbest estimator lgbm's best error=0.0908\n",
      "[flaml.automl.logger: 04-06 21:06:07] {2258} INFO - iteration 18, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:08] {2442} INFO -  at 7.7s,\testimator lgbm's best error=0.0908,\tbest estimator lgbm's best error=0.0908\n",
      "[flaml.automl.logger: 04-06 21:06:08] {2258} INFO - iteration 19, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:09] {2442} INFO -  at 8.7s,\testimator lgbm's best error=0.0908,\tbest estimator lgbm's best error=0.0908\n",
      "[flaml.automl.logger: 04-06 21:06:09] {2258} INFO - iteration 20, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:12] {2442} INFO -  at 12.3s,\testimator lgbm's best error=0.0836,\tbest estimator lgbm's best error=0.0836\n",
      "[flaml.automl.logger: 04-06 21:06:12] {2258} INFO - iteration 21, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:19] {2442} INFO -  at 19.1s,\testimator lgbm's best error=0.0836,\tbest estimator lgbm's best error=0.0836\n",
      "[flaml.automl.logger: 04-06 21:06:19] {2258} INFO - iteration 22, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:21] {2442} INFO -  at 20.5s,\testimator lgbm's best error=0.0836,\tbest estimator lgbm's best error=0.0836\n",
      "[flaml.automl.logger: 04-06 21:06:21] {2258} INFO - iteration 23, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:25] {2442} INFO -  at 25.1s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:06:25] {2258} INFO - iteration 24, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:29] {2442} INFO -  at 28.6s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:06:29] {2258} INFO - iteration 25, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:35] {2442} INFO -  at 34.9s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:06:35] {2258} INFO - iteration 26, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:37] {2442} INFO -  at 36.6s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:06:37] {2258} INFO - iteration 27, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:45] {2442} INFO -  at 45.0s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:06:45] {2258} INFO - iteration 28, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:47] {2442} INFO -  at 47.0s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:06:47] {2258} INFO - iteration 29, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:49] {2442} INFO -  at 48.4s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:06:49] {2258} INFO - iteration 30, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:06:58] {2442} INFO -  at 57.4s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:06:58] {2258} INFO - iteration 31, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:07:01] {2442} INFO -  at 60.6s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:07:01] {2258} INFO - iteration 32, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:07:07] {2442} INFO -  at 66.9s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:07:07] {2258} INFO - iteration 33, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:07:11] {2442} INFO -  at 70.8s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:07:11] {2258} INFO - iteration 34, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:07:15] {2442} INFO -  at 74.4s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:07:15] {2258} INFO - iteration 35, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:07:29] {2442} INFO -  at 89.2s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:07:29] {2258} INFO - iteration 36, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:07:31] {2442} INFO -  at 90.4s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:07:31] {2258} INFO - iteration 37, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:07:38] {2442} INFO -  at 98.1s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:07:38] {2258} INFO - iteration 38, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:07:40] {2442} INFO -  at 100.2s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:07:40] {2258} INFO - iteration 39, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:07:53] {2442} INFO -  at 112.4s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:07:53] {2258} INFO - iteration 40, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:07:53] {2442} INFO -  at 113.2s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:07:53] {2258} INFO - iteration 41, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:07:54] {2442} INFO -  at 114.0s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:07:54] {2258} INFO - iteration 42, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:08:13] {2442} INFO -  at 133.1s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:08:13] {2258} INFO - iteration 43, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:08:21] {2442} INFO -  at 140.5s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:08:21] {2258} INFO - iteration 44, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:08:22] {2442} INFO -  at 142.3s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:08:22] {2258} INFO - iteration 45, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:08:23] {2442} INFO -  at 143.2s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:08:23] {2258} INFO - iteration 46, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:08:46] {2442} INFO -  at 166.0s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:08:46] {2258} INFO - iteration 47, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:08:47] {2442} INFO -  at 166.7s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:08:47] {2258} INFO - iteration 48, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:09:12] {2442} INFO -  at 191.7s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:09:12] {2258} INFO - iteration 49, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:09:12] {2442} INFO -  at 192.3s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:09:12] {2258} INFO - iteration 50, current learner lgbm\n",
      "[flaml.automl.logger: 04-06 21:09:20] {2442} INFO -  at 200.2s,\testimator lgbm's best error=0.0808,\tbest estimator lgbm's best error=0.0808\n",
      "[flaml.automl.logger: 04-06 21:09:21] {2685} INFO - retrain lgbm for 0.8s\n",
      "[flaml.automl.logger: 04-06 21:09:21] {2688} INFO - retrained model: LGBMClassifier(colsample_bytree=0.7087614338457834,\n",
      "               learning_rate=0.08779515636942332, max_bin=511,\n",
      "               min_child_samples=6, n_estimators=57, n_jobs=-1, num_leaves=198,\n",
      "               reg_alpha=0.001346442339014509, reg_lambda=0.00839141933486936,\n",
      "               verbose=-1)\n",
      "[flaml.automl.logger: 04-06 21:09:21] {1985} INFO - fit succeeded\n",
      "[flaml.automl.logger: 04-06 21:09:21] {1986} INFO - Time taken to find the best model: 25.142093658447266\n"
     ]
    }
   ],
   "source": [
    "#Train model using Flaml AutoML\n",
    "\n",
    "model_config = {\n",
    "    'task' : 'classification',  # classification \n",
    "    'time_budget' : 200,    # time budget in seconds\n",
    "    'metric' : 'f1', # main metric to be optimized\n",
    "    'estimator_list' : ['lgbm'] ,\n",
    "    'eval_method': 'cv',  \n",
    "    'n_splits': 5,\n",
    "}\n",
    "\n",
    "model = AutoML()\n",
    "model.fit(X_res, y_res, **model_config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lgbm': {'n_estimators': 57,\n",
       "  'num_leaves': 198,\n",
       "  'min_child_samples': 6,\n",
       "  'learning_rate': 0.08779515636942332,\n",
       "  'log_max_bin': 9,\n",
       "  'colsample_bytree': 0.7087614338457834,\n",
       "  'reg_alpha': 0.001346442339014509,\n",
       "  'reg_lambda': 0.00839141933486936}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view the best configuration for each ML model\n",
    "model.best_config_per_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.94      0.92       981\n",
      "         1.0       0.54      0.40      0.46       181\n",
      "\n",
      "    accuracy                           0.85      1162\n",
      "   macro avg       0.72      0.67      0.69      1162\n",
      "weighted avg       0.84      0.85      0.85      1162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Display metrics\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.8246\n",
      "PR-AUC: 0.5050\n",
      "Accuracy: 0.8546\n",
      "Precision: 0.5448\n",
      "Recall: 0.4033\n",
      "F1: 0.4635\n"
     ]
    }
   ],
   "source": [
    "#Evaluate model performance\n",
    "from typing import Dict, Any\n",
    "def evaluate_model(y_true, y_pred_proba, threshold: float = 0.5) -> Dict[str, float]:\n",
    "    metrics = {\n",
    "        'ROC-AUC': {'name': 'ROC-AUC', 'kwargs': {}},\n",
    "        'PR-AUC': {'name': 'PR-AUC', 'kwargs': {}},\n",
    "        'Accuracy': {'name': 'Accuracy', 'kwargs': {'threshold': threshold}},\n",
    "        'Precision': {'name': 'Precision', 'kwargs': {'threshold': threshold}},\n",
    "        'Recall': {'name': 'Recall', 'kwargs': {'threshold': threshold}},\n",
    "        'F1': {'name': 'F1', 'kwargs': {'threshold': threshold}}\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for metric_name, config in metrics.items():\n",
    "        evaluator = Evaluator(name=config['name'])\n",
    "        score = evaluator(y_true, y_pred_proba, **config['kwargs'])\n",
    "        results[metric_name] = score\n",
    "        print(f\"{metric_name}: {score:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "y_true = y_test\n",
    "\n",
    "evaluation_results = evaluate_model(y_true, y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Drugtax_trained_model.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save model\n",
    "\n",
    "model = model\n",
    "model_filename = 'Drugtax_trained_model.joblib'\n",
    "joblib.dump(model, model_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
